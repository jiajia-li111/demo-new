import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import joblib


# ===================== 1. æ•°æ®åŠ è½½ =====================
def load_data(file_path):
    """åŠ è½½æ•°æ®é›†"""
    try:
        df = pd.read_csv(file_path)
        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®ï¼š{file_path}")
        df.info()

        rows, cols = df.shape
        print(f"æ•°æ®é›†åŒ…å« {rows} è¡Œï¼Œ{cols} åˆ—")

        if rows < 100:
            print(f"âš ï¸ æ ·æœ¬æ•°è¾ƒå°‘ï¼ˆ{rows} æ¡ï¼‰ï¼Œå¯èƒ½å½±å“æ¨¡å‹æ³›åŒ–èƒ½åŠ›")

        if df.isnull().sum().sum() > 0:
            print("âš ï¸ æ•°æ®é›†ä¸­å­˜åœ¨ç¼ºå¤±å€¼ï¼Œå°†åœ¨åç»­æ­¥éª¤ä¸­åˆ é™¤")
        else:
            print("âœ… æ•°æ®é›†ä¸­ä¸å­˜åœ¨ç¼ºå¤±å€¼")

        return df
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ–‡ä»¶ {file_path}")
        return None
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®æ—¶å‡ºç°å¼‚å¸¸ï¼š{e}")
        return None


# ===================== 2. æ•°æ®é¢„å¤„ç† =====================
def preprocess_data(df, out_dir='train/run_out'):
    """é¢„å¤„ç†æ•°æ®"""
    if df is None:
        print("âŒ æ•°æ®ä¸ºç©ºï¼Œæ— æ³•é¢„å¤„ç†ã€‚")
        return None, None, None, None, None, None

    df = df.dropna()
    X = df.iloc[:, :-1]
    y = df.iloc[:, -1]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    feature_names = X.columns.tolist()

    os.makedirs(out_dir, exist_ok=True)
    joblib.dump(scaler, f'{out_dir}/scaler.pkl')
    joblib.dump(feature_names, f'{out_dir}/feature_names.pkl')
    print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œå·²ä¿å­˜ scaler å’Œ feature_names è‡³ {out_dir}")

    return X_train_scaled, X_test_scaled, y_train, y_test, X_train, X_test


# ===================== 3. æ¨¡å‹è®­ç»ƒ =====================
def train_model(X_train_scaled, y_train, n_estimators=100, random_state=42):
    """è®­ç»ƒéšæœºæ£®æ—åˆ†ç±»å™¨"""
    if X_train_scaled is None or y_train is None:
        print("âŒ æ— æ³•è®­ç»ƒæ¨¡å‹ï¼Œè¾“å…¥æ•°æ®ä¸ºç©ºã€‚")
        return None

    model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)
    model.fit(X_train_scaled, y_train)
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
    return model


# ===================== 4. ä¿å­˜æ¨¡å‹ =====================
def save_model(model, model_path):
    """ä¿å­˜æ¨¡å‹"""
    if model is not None:
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        joblib.dump(model, model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³ï¼š{model_path}")


# ===================== 5. åŠ è½½æ¨¡å‹ =====================
def load_model(model_path):
    """åŠ è½½æ¨¡å‹"""
    try:
        model = joblib.load(model_path)
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹ï¼š{model_path}")
        return model
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ï¼š{model_path}")
        return None
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å‡ºç°å¼‚å¸¸ï¼š{e}")
        return None


# ===================== 6. æ¨¡å‹è¯„ä¼° =====================
def evaluate_model(model, X_test_scaled, y_test):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    if model is None or X_test_scaled is None or y_test is None:
        print("âŒ è¯„ä¼°å¤±è´¥ï¼šæ¨¡å‹æˆ–æµ‹è¯•æ•°æ®ä¸ºç©ºã€‚")
        return None

    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ğŸ¯ æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.4f}")
    print("åˆ†ç±»æŠ¥å‘Šï¼š")
    print(classification_report(y_test, y_pred))
    return y_pred


# ===================== 7. ç‰¹å¾é‡è¦æ€§åˆ†æ =====================
def analyze_feature_importance(model, X_train, out_dir='train/run_out', top_n=10):
    """åˆ†æç‰¹å¾é‡è¦æ€§"""
    if model is None or X_train is None:
        return None

    importance = model.feature_importances_
    feature_names = X_train.columns

    importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importance
    }).sort_values('Importance', ascending=False).head(top_n)

    os.makedirs(out_dir, exist_ok=True)
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=importance_df)
    plt.title('ç‰¹å¾é‡è¦æ€§åˆ†æ')
    plt.tight_layout()
    plt.savefig(f'{out_dir}/feature_importance.png')
    plt.close()

    print(f"ğŸ“Š ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜è‡³ {out_dir}/feature_importance.png")
    return importance_df


# ===================== 8. å¯è§†åŒ–ç»“æœ =====================
def visualize_results(y_test, y_pred, out_dir='train/run_out'):
    """å¯è§†åŒ–æ··æ·†çŸ©é˜µ"""
    if y_test is None or y_pred is None:
        return

    cm = confusion_matrix(y_test, y_pred)
    os.makedirs(out_dir, exist_ok=True)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.xlabel('é¢„æµ‹å€¼')
    plt.ylabel('çœŸå®å€¼')
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.tight_layout()
    plt.savefig(f'{out_dir}/confusion_matrix.png')
    plt.close()
    print(f"ğŸ“ˆ æ··æ·†çŸ©é˜µå›¾å·²ä¿å­˜è‡³ {out_dir}/confusion_matrix.png")


# ===================== 9. é¢„æµ‹å‡½æ•° =====================
def detect_disease(
    input_data,
    model_path,
    scaler_path,
    feature_names_path
):
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    model = load_model(model_path)
    if model is None:
        return None

    scaler = joblib.load(scaler_path)
    feature_names = joblib.load(feature_names_path)

    if not isinstance(input_data, pd.DataFrame):
        input_data = pd.DataFrame([input_data], columns=feature_names)

    input_scaled = scaler.transform(input_data)
    prediction = model.predict(input_scaled)
    prediction_proba = model.predict_proba(input_scaled)

    return {
        'prediction': prediction[0],
        'probability': prediction_proba[0].tolist()
    }


# ===================== 10. ä¸»å‡½æ•° =====================
def main():
    # === è‡ªåŠ¨è¯†åˆ«è¦ä½¿ç”¨çš„æ•°æ®é›† ===
    base_dir = os.path.dirname(__file__)
    data_dir = os.path.join(base_dir, "data")

    # ä½ å¯ä»¥æ¢æˆä¸‹é¢ä»»æ„ä¸€ä¸ªæ–‡ä»¶åå³å¯ï¼š
    dataset_name = "heart_failure_reduced.csv"  # âœ… å¯æ”¹æˆ diabetes_reduced.csv
    file_path = os.path.join(data_dir, dataset_name)

    # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºç›®å½•ï¼ˆæŒ‰æ•°æ®é›†åç§°åŒºåˆ†ï¼‰
    dataset_short = os.path.splitext(dataset_name)[0]
    out_dir = os.path.join(base_dir, "run_out", dataset_short)

    print(f"\nğŸ” å½“å‰æ•°æ®é›†ï¼š{dataset_name}")
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶è·¯å¾„ï¼š{file_path}")

    df = load_data(file_path)
    if df is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    X_train_scaled, X_test_scaled, y_train, y_test, X_train, X_test = preprocess_data(df, out_dir=out_dir)
    model = train_model(X_train_scaled, y_train)
    model_path = os.path.join(out_dir, f"{dataset_short}_model.pkl")

    save_model(model, model_path)
    y_pred = evaluate_model(model, X_test_scaled, y_test)

    importance_df = analyze_feature_importance(model, X_train, out_dir=out_dir)
    if importance_df is not None:
        print("\nç‰¹å¾é‡è¦æ€§å‰10ï¼š")
        print(importance_df)

    visualize_results(y_test, y_pred, out_dir=out_dir)

    # ç¤ºä¾‹é¢„æµ‹
    sample_input = X_test.iloc[0].tolist()
    result = detect_disease(
        sample_input,
        model_path=model_path,
        scaler_path=os.path.join(out_dir, "scaler.pkl"),
        feature_names_path=os.path.join(out_dir, "feature_names.pkl")
    )
    if result:
        print("\nğŸ§  ç¤ºä¾‹é¢„æµ‹ç»“æœï¼š")
        print(f"é¢„æµ‹ç±»åˆ«: {result['prediction']}")
        print(f"é¢„æµ‹æ¦‚ç‡: {result['probability']}")


if __name__ == "__main__":
    main()
